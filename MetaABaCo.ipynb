{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is MetaABaCo: A Variational Autoencoder Batch Correction algorithm. It uses the Encoder to extract only biological relevant information from the data while keeping apart batch variability from it. This is achieved by using a batch discriminator for adversarial training. The batch would try to correcly classify the batch label from the encoder output (latent space) while the encoder would try to confuse the discriminator by leaving appart as much batch information as possible. The decoder would work as a data recovery step, where it would reconstruct the data from the latent space. The prior distribution used is the Mixture of Gaussians (MoG) that can be useful for the latent space to retain the biological significance at all times. The biological significance is also assured to be retained by training a classifier that is able to distinguish the cofounding variable labels from the samples on the latent space. This is an attempt to make the original ABaCo algorithm, which is based on a simple autoencoder, into a probabilistic approach that tries to resemble the data on distributions divergence rather than MSE losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, kl_divergence, NegativeBinomial, Bernoulli, Categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from umap import UMAP\n",
    "\n",
    "#User libraries\n",
    "from BatchEffectDataLoader import DataPreprocess, DataTransform, ABaCoDataLoader\n",
    "from BatchEffectCorrection import correctCombat\n",
    "from BatchEffectPlots import plotPCA\n",
    "from BatchEffectMetrics import kBET, iLISI, cLISI, ARI, ASW\n",
    "from ABaCo import ABaCo, BatchDiscriminator, TissueClassifier, DataDiscriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class for the ABaCo model\n",
    "class ABaCoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, batch_size, d_z, n_comp,\n",
    "                 hl1 = 1028, hl2 = 512, hl3 = 256):\n",
    "        super().__init__()\n",
    "        self.d_z = d_z\n",
    "        self.n_comp = n_comp\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(input_size + batch_size, hl1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl1, hl2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl2, hl3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hl3, d_z * n_comp)\n",
    "        self.fc_logvar = nn.Linear(hl3, d_z * n_comp)\n",
    "        self.fc_pi = nn.Linear(hl3, n_comp)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        mu = self.fc_mu(encoded).view(-1, self.d_z, self.n_comp)\n",
    "        logvar = self.fc_logvar(encoded).view(-1, self.d_z, self.n_comp)\n",
    "        pi = nn.functional.softmax(self.fc_pi(encoded), dim=-1)\n",
    "        return mu, logvar, pi\n",
    "\n",
    "#Decoder class for the ABaCo model - ZINB is used, so the output would be the parameters of the distribution\n",
    "class ABaCoDecoder(nn.Module):\n",
    "    def __init__(self, output_size, d_z, n_comp,\n",
    "                 hl1 = 256, hl2 = 512, hl3 = 1028):\n",
    "        super().__init__()\n",
    "        self.d_z = d_z\n",
    "        self.n_comp = n_comp\n",
    "        self.output_size = output_size\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(d_z, hl1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl1, hl2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl2, hl3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hl3, output_size)\n",
    "        self.fc_logtheta = nn.Linear(hl3, output_size)\n",
    "        self.fc_pi = nn.Linear(hl3, output_size)\n",
    "    \n",
    "    # Reparameterization trick to sample z ~ MoG(z |Â mu, logvar, pi)\n",
    "    def reparameterize(self, mu, logvar, pi):\n",
    "\n",
    "        z_sample = []\n",
    "\n",
    "        for n in range(self.n_comp):\n",
    "            \n",
    "            n_logvar = logvar.gather[:, n, :]\n",
    "            n_mu = mu.gather[:, n, :]\n",
    "\n",
    "            std = torch.exp(0.5*n_logvar)\n",
    "            eps = torch.radn_like(std)\n",
    "            z = n_mu + eps*std\n",
    "            z_sample.append(pi[:, n].view(-1,1)*z)\n",
    "        \n",
    "        return torch.sum(torch.stack(z_sample), dim=0)\n",
    "    \n",
    "    def forward(self, mu, logvar, pi):\n",
    "        z = self.reparameterize(mu, logvar, pi)\n",
    "        decoded = self.decode(z)\n",
    "        mu_nb = torch.exp(self.fc_mu(decoded)) # Exp to ensure positive mu\n",
    "        logtheta = self.fc_logtheta(decoded) # Log-dispersion of NB\n",
    "        pi_nb = torch.sigmoid(self.fc_pi(decoded))\n",
    "\n",
    "        return mu_nb, logtheta, pi_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction loss for ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OTU data have zero-inflation and over-dispersion, which means that the variance is by far greater than the mean. The Negative Binomial distribution accounts for that and it's assume to be a good fit to model the OTU data. This is also used in the scDREAMER to account for zero-inflation in single cell RNA seq data. We are going to try to apply a variation of it called Zero-Inflated Negative Binomial distribution (ZINB), and we are going to try model it with non-transformed OTU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the negative log-likelihood of the zero-inflated negative binomial distribution\n",
    "def zinb_nll(x, mu, logtheta, pi):\n",
    "    # Negative Binomial likelihood\n",
    "    nb_dist = NegativeBinomial(mu, torch.exp(logtheta))\n",
    "    zinb_dist = Bernoulli(pi)\n",
    "\n",
    "    #Log-likelihood of ZINB: p(x) = pi * Bernoulli(x = 0) + (1 - pi) * NB(x | mu, theta)\n",
    "    ll = torch.log(pi * zinb_dist.log_prob(torch.zeros_like(x)) + (1 - pi) * nb_dist.log_prob(x))\n",
    "    nll = -torch.sum(ll)\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence for MoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using a MoG model as our prior is a bit tricky to actually compute the KL-divergence of the posterior to the prior. For that, let's start by defining the KL-divergence definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructed data from ZINB distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the parameters of the ZINB, we are going to reconstruct the OTU data using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_zinb(mu, logtheta, pi):\n",
    "    \n",
    "    nb_dist = NegativeBinomial(mu, torch.exp(logtheta))\n",
    "    zinb_dist = Bernoulli(pi)\n",
    "\n",
    "    #1st sample from Bernoulli\n",
    "    bernoulli_sample = zinb_dist.sample()\n",
    "\n",
    "    #2nd sample from NB\n",
    "    nb_sample = nb_dist.sample()\n",
    "\n",
    "    #3rd apply ZI to NB\n",
    "    recon_data = bernoulli_sample * torch.zeros_like(nb_sample) + (1 - bernoulli_sample)*nb_sample\n",
    "\n",
    "    return recon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for the model\n",
    "def train_MetaABaCo(\n",
    "        encoder: ABaCoEncoder,\n",
    "        decoder: ABaCoDecoder,\n",
    "        train_loader,\n",
    "        num_epochs,\n",
    "        device,\n",
    "        lr_encode = 1e-5,\n",
    "        lr_decode = 1e-5\n",
    "):\n",
    "    encoder_optim = torch.optim.Adam(encoder.parameters(), lr = lr_encode, weight_decay=1e-5)\n",
    "    decoder_optim = torch.optim.Adam(decoder.parameters(), lr = lr_decode, weight_decay=1e-5)\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        train_loss = 0\n",
    "\n",
    "        for x, _, _ in train_loader:\n",
    "            #Forward pass to encoder\n",
    "            x = x.to(device)\n",
    "            mu, logvar, pi = encoder(x)\n",
    "\n",
    "            #Forward pass to decoder\n",
    "            mu_nb, logtheta, pi_zinb = decoder(mu, logvar, pi)\n",
    "\n",
    "            #Reconstructed OTU data\n",
    "            recon_data = sample_from_zinb(mu_nb, logtheta, pi_zinb)\n",
    "\n",
    "            #Reconstruction loss\n",
    "            recon_loss = zinb_nll(x, mu_nb, logtheta, pi_zinb)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BECA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
