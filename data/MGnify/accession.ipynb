{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonapi_client import Session\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global surveillance of antimicrobial resistance (DTU-GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set study accession\n",
    "study_accession = \"MGYS00001312\"\n",
    "\n",
    "# Create session with MGnify API endpoint\n",
    "with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    # Iterate over all analyses in study\n",
    "    analyses_iter = mgnify.iterate(f\"studies/{study_accession}/analyses\")\n",
    "    # Extract JSON from each record\n",
    "    analyses_json = [record.json for record in analyses_iter]\n",
    "    # Normalize HSON into pd.DataFrame\n",
    "    df = pd.json_normalize(analyses_json)\n",
    "\n",
    "analysis_ids = df[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "with tqdm(total=len(analysis_ids), desc=\"Fetching Taxonomy SSU Data\", unit=\"analysis\") as pbar:\n",
    "    for analysis_id in analysis_ids:\n",
    "\n",
    "        url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/analyses/{analysis_id}/taxonomy/ssu\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            try:\n",
    "                data = response.json()\n",
    "                df_temp = pd.json_normalize(data[\"data\"])\n",
    "            \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error processing JSON for {analysis_id}: {e}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            df_temp[\"analysis_id\"] = analysis_id\n",
    "            dfs.append(df_temp)\n",
    "\n",
    "        else:\n",
    "            tqdm.write(f\"Error fetching data for {analysis_id}: HTTP {response.status_code}\")\n",
    "        \n",
    "        pbar.set_postfix_str(f\"Remaining: {len(analysis_ids) - pbar.n - 1}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "if dfs:\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"Combined DataFrame:\")\n",
    "    final_df.head\n",
    "\n",
    "else:\n",
    "    print(\"No data was retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting metadata from each analysis id sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = []\n",
    "\n",
    "with tqdm(total=len(analysis_ids), desc=\"Fetching Analysis Sample Data\", unit=\"analysis\") as pbar:\n",
    "    for analysis_id in analysis_ids:\n",
    "        url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/analyses/{analysis_id}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                sample_id = data[\"data\"][\"relationships\"][\"sample\"][\"data\"][\"id\"]\n",
    "                sample_url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/samples/{sample_id}\"\n",
    "                sample_response = requests.get(sample_url)\n",
    "\n",
    "                if sample_response.status_code == 200:\n",
    "                    try:\n",
    "                        sample_data = sample_response.json()\n",
    "                        sample_attributes = sample_data[\"data\"][\"attributes\"]\n",
    "                        \n",
    "                        # Extract geographic location from sample-metadata\n",
    "                        geographic_location = None\n",
    "                        sample_metadata_list = sample_attributes.get(\"sample-metadata\", [])\n",
    "                        \n",
    "                        # Look for keys containing \"geographic location\"\n",
    "                        for entry in sample_metadata_list:\n",
    "                            key = entry.get(\"key\", \"\").lower()\n",
    "                            if \"country\" in key:\n",
    "                                geographic_location = entry.get(\"value\")\n",
    "                                break  # Stop after first match\n",
    "\n",
    "                        sample_metadata = {\n",
    "                            \"analysis_id\": analysis_id,\n",
    "                            \"sample_id\": sample_id,\n",
    "                            \"sample_name\": sample_attributes.get(\"sample-name\"),\n",
    "                            \"collection_date\": sample_attributes.get(\"collection-date\"),\n",
    "                            \"geographic_location\": geographic_location,\n",
    "                        }\n",
    "                        \n",
    "                        metadata_df.append(sample_metadata)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        tqdm.write(f\"Error processing sample JSON for {sample_id}: {e}\")\n",
    "                \n",
    "                else:\n",
    "                    tqdm.write(f\"Error fetching sample {sample_id}: HTTP {sample_response.status_code}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error processing analysis JSON for {analysis_id}: {e}\")\n",
    "        \n",
    "        else:\n",
    "            tqdm.write(f\"Error fetching analysis {analysis_id}: HTTP {response.status_code}\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "final_metadata_df = pd.DataFrame(metadata_df) if metadata_df else pd.DataFrame()\n",
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define taxonomic rank and creating count data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = \"family\"\n",
    "\n",
    "ranked_df = final_df[(final_df[f\"attributes.hierarchy.{rank}\"].notna()) & \n",
    "                      (final_df[f\"attributes.hierarchy.{rank}\"] != '')\n",
    "                      ]\n",
    "\n",
    "grouped_df = ranked_df.groupby([\"analysis_id\", f\"attributes.hierarchy.{rank}\"],\n",
    "                               as_index=False,\n",
    "                               )[\"attributes.count\"].sum()\n",
    "\n",
    "wide_df = grouped_df.pivot_table(\n",
    "    index=\"analysis_id\",\n",
    "    columns=f\"attributes.hierarchy.{rank}\",\n",
    "    values=\"attributes.count\",\n",
    "    fill_value = 0\n",
    ").reset_index()\n",
    "\n",
    "final_merged_df = final_metadata_df.merge(\n",
    "    wide_df,\n",
    "    on=\"analysis_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_merged_df.to_csv(\"datasets/Global_surveillance/MGYS00005846_taxon_family.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BECA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
