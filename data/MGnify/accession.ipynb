{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonapi_client import Session\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for asynchronous requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_taxonomy(session, analysis_id):\n",
    "    url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/analyses/{analysis_id}/taxonomy/ssu\"\n",
    "    async with session.get(url) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            df_temp = pd.json_normalize(data[\"data\"])\n",
    "            df_temp[\"analysis_id\"] = analysis_id\n",
    "            return df_temp\n",
    "        else:\n",
    "            print(f\"Error fetching data for {analysis_id}: HTTP {response.status}\")\n",
    "            return None\n",
    "\n",
    "async def fetch_metadata(session, analysis_id):\n",
    "    url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/analyses/{analysis_id}\"\n",
    "    async with session.get(url) as response:\n",
    "        if response.status == 200:\n",
    "            data = await response.json()\n",
    "            try:\n",
    "                sample_id = data[\"data\"][\"relationships\"][\"sample\"][\"data\"][\"id\"]\n",
    "                sample_url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/samples/{sample_id}\"\n",
    "                async with session.get(sample_url) as sample_response:\n",
    "                    if sample_response.status == 200:\n",
    "                        sample_data = await sample_response.json()\n",
    "                        sample_attributes = sample_data[\"data\"][\"attributes\"]\n",
    "                        geographic_location = None\n",
    "                        for entry in sample_attributes.get(\"sample-metadata\", []):\n",
    "                            key = entry.get(\"key\", \"\").lower()\n",
    "                            if \"country\" in key:\n",
    "                                geographic_location = entry.get(\"value\")\n",
    "                                break\n",
    "                        return {\n",
    "                            \"analysis_id\": analysis_id,\n",
    "                            \"sample_id\": sample_id,\n",
    "                            \"sample_name\": sample_attributes.get(\"sample-name\"),\n",
    "                            \"collection_date\": sample_attributes.get(\"collection-date\"),\n",
    "                            \"geographic_location\": geographic_location,\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"Error fetching sample {sample_id}: HTTP {sample_response.status}\")\n",
    "                        return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing analysis JSON for {analysis_id}: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Error fetching analysis {analysis_id}: HTTP {response.status}\")\n",
    "            return None\n",
    "\n",
    "async def main(analysis_ids):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Schedule all requests concurrently\n",
    "        taxonomy_tasks = [fetch_taxonomy(session, aid) for aid in analysis_ids]\n",
    "        metadata_tasks = [fetch_metadata(session, aid) for aid in analysis_ids]\n",
    "\n",
    "        taxonomy_results = await asyncio.gather(*taxonomy_tasks)\n",
    "        metadata_results = await asyncio.gather(*metadata_tasks)\n",
    "\n",
    "        # Filter out any failed (None) results\n",
    "        taxonomy_dfs = [res for res in taxonomy_results if res is not None]\n",
    "        metadata_list = [res for res in metadata_results if res is not None]\n",
    "\n",
    "        taxonomy_df = pd.concat(taxonomy_dfs, ignore_index=True) if taxonomy_dfs else pd.DataFrame()\n",
    "        metadata_df = pd.DataFrame(metadata_list) if metadata_list else pd.DataFrame()\n",
    "\n",
    "        return taxonomy_df, metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global surveillance of antimicrobial resistance (DTU-GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set study accession\n",
    "study_accession = \"MGYS00001312\"\n",
    "\n",
    "# Create session with MGnify API endpoint\n",
    "with Session(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    # Iterate over all analyses in study\n",
    "    analyses_iter = mgnify.iterate(f\"studies/{study_accession}/analyses\")\n",
    "    # Extract JSON from each record\n",
    "    analyses_json = [record.json for record in analyses_iter]\n",
    "    # Normalize HSON into pd.DataFrame\n",
    "    df = pd.json_normalize(analyses_json)\n",
    "\n",
    "analysis_ids = df[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Taxonomy SSU Data: 100%|██████████| 413/413 [41:26<00:00,  6.02s/analysis, Remaining: 0]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame:\n"
     ]
    }
   ],
   "source": [
    "# dfs = []\n",
    "\n",
    "# with tqdm(total=len(analysis_ids), desc=\"Fetching Taxonomy SSU Data\", unit=\"analysis\") as pbar:\n",
    "#     for analysis_id in analysis_ids:\n",
    "\n",
    "#         url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/analyses/{analysis_id}/taxonomy/ssu\"\n",
    "#         response = requests.get(url)\n",
    "\n",
    "#         if response.status_code == 200:\n",
    "\n",
    "#             try:\n",
    "#                 data = response.json()\n",
    "#                 df_temp = pd.json_normalize(data[\"data\"])\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 tqdm.write(f\"Error processing JSON for {analysis_id}: {e}\")\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "\n",
    "#             df_temp[\"analysis_id\"] = analysis_id\n",
    "#             dfs.append(df_temp)\n",
    "\n",
    "#         else:\n",
    "#             tqdm.write(f\"Error fetching data for {analysis_id}: HTTP {response.status_code}\")\n",
    "        \n",
    "#         pbar.set_postfix_str(f\"Remaining: {len(analysis_ids) - pbar.n - 1}\")\n",
    "#         pbar.update(1)\n",
    "\n",
    "# if dfs:\n",
    "#     final_df = pd.concat(dfs, ignore_index=True)\n",
    "#     print(\"Combined DataFrame:\")\n",
    "#     final_df.head\n",
    "\n",
    "# else:\n",
    "#     print(\"No data was retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting metadata from each analysis id sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Analysis Sample Data:  53%|█████▎    | 220/413 [44:02<23:40,  7.36s/analysis]  "
     ]
    }
   ],
   "source": [
    "# metadata_df = []\n",
    "\n",
    "# with tqdm(total=len(analysis_ids), desc=\"Fetching Analysis Sample Data\", unit=\"analysis\") as pbar:\n",
    "#     for analysis_id in analysis_ids:\n",
    "#         url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/analyses/{analysis_id}\"\n",
    "#         response = requests.get(url)\n",
    "\n",
    "#         if response.status_code == 200:\n",
    "#             try:\n",
    "#                 data = response.json()\n",
    "#                 sample_id = data[\"data\"][\"relationships\"][\"sample\"][\"data\"][\"id\"]\n",
    "#                 sample_url = f\"https://www.ebi.ac.uk/metagenomics/api/v1/samples/{sample_id}\"\n",
    "#                 sample_response = requests.get(sample_url)\n",
    "\n",
    "#                 if sample_response.status_code == 200:\n",
    "#                     try:\n",
    "#                         sample_data = sample_response.json()\n",
    "#                         sample_attributes = sample_data[\"data\"][\"attributes\"]\n",
    "                        \n",
    "#                         # Extract geographic location from sample-metadata\n",
    "#                         geographic_location = None\n",
    "#                         sample_metadata_list = sample_attributes.get(\"sample-metadata\", [])\n",
    "                        \n",
    "#                         # Look for keys containing \"geographic location\"\n",
    "#                         for entry in sample_metadata_list:\n",
    "#                             key = entry.get(\"key\", \"\").lower()\n",
    "#                             if \"country\" in key:\n",
    "#                                 geographic_location = entry.get(\"value\")\n",
    "#                                 break  # Stop after first match\n",
    "\n",
    "#                         sample_metadata = {\n",
    "#                             \"analysis_id\": analysis_id,\n",
    "#                             \"sample_id\": sample_id,\n",
    "#                             \"sample_name\": sample_attributes.get(\"sample-name\"),\n",
    "#                             \"collection_date\": sample_attributes.get(\"collection-date\"),\n",
    "#                             \"geographic_location\": geographic_location,\n",
    "#                         }\n",
    "                        \n",
    "#                         metadata_df.append(sample_metadata)\n",
    "                    \n",
    "#                     except Exception as e:\n",
    "#                         tqdm.write(f\"Error processing sample JSON for {sample_id}: {e}\")\n",
    "                \n",
    "#                 else:\n",
    "#                     tqdm.write(f\"Error fetching sample {sample_id}: HTTP {sample_response.status_code}\")\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 tqdm.write(f\"Error processing analysis JSON for {analysis_id}: {e}\")\n",
    "        \n",
    "#         else:\n",
    "#             tqdm.write(f\"Error fetching analysis {analysis_id}: HTTP {response.status_code}\")\n",
    "        \n",
    "#         pbar.update(1)\n",
    "\n",
    "# final_metadata_df = pd.DataFrame(metadata_df) if metadata_df else pd.DataFrame()\n",
    "# final_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asyncio - extraction of data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run the asynchronous main function\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m taxonomy_df, metadata_df \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalysis_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombined Taxonomy DataFrame:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(taxonomy_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/miniforge3/envs/BECA/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/miniforge3/envs/BECA/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/BECA/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/miniforge3/envs/BECA/lib/python3.12/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming `df` from your study analysis retrieval code:\n",
    "analysis_ids = df[\"id\"].tolist()\n",
    "\n",
    "# Patch the event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the asynchronous main function\n",
    "taxonomy_df, metadata_df = asyncio.run(main(analysis_ids))\n",
    "\n",
    "print(\"Combined Taxonomy DataFrame:\")\n",
    "print(taxonomy_df.head())\n",
    "print(\"\\nMetadata DataFrame:\")\n",
    "print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define taxonomic rank and creating count data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = \"family\"\n",
    "\n",
    "ranked_df = final_df[(final_df[f\"attributes.hierarchy.{rank}\"].notna()) & \n",
    "                      (final_df[f\"attributes.hierarchy.{rank}\"] != '')\n",
    "                      ]\n",
    "\n",
    "grouped_df = ranked_df.groupby([\"analysis_id\", f\"attributes.hierarchy.{rank}\"],\n",
    "                               as_index=False,\n",
    "                               )[\"attributes.count\"].sum()\n",
    "\n",
    "wide_df = grouped_df.pivot_table(\n",
    "    index=\"analysis_id\",\n",
    "    columns=f\"attributes.hierarchy.{rank}\",\n",
    "    values=\"attributes.count\",\n",
    "    fill_value = 0\n",
    ").reset_index()\n",
    "\n",
    "final_merged_df = final_metadata_df.merge(\n",
    "    wide_df,\n",
    "    on=\"analysis_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df.to_csv(\"datasets/Global_surveillance/MGYS00001312_taxon_family.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BECA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
