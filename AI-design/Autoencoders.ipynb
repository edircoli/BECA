{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f7300f-800d-4353-bfae-108f33fb196f",
   "metadata": {},
   "source": [
    "# Autoencoder Pytorch Implementation\n",
    "\n",
    "## Description\n",
    "\n",
    "In this notebook we implemented a different latent variable models that are all members of the autoencoder model family.\n",
    "An autoencoder model is an unsupervised machine learning model, which is trained on **unlabelled data**. The models are trained to compress\n",
    "data $x$ from a high dimensional space $m$ to a lower dimensional space $n$. The compressed data $z$ is called a latent variable, because\n",
    "it is an unobserved variable/latent/hidden, compared to the observed variable $x$. Why do we bother training models that compress and uncompress data?\n",
    "To give you some intuition, we can think about the process of compression and decompression as a solution to the problem of keeping only the \n",
    "minimum necessary information to describe the entire object. We aim to make the representation space small enough, that it is challenging for \n",
    "the model to reconstruct the data, but large enough that it contains all the necessary information to achieve the task. By forcing\n",
    "the model to compress the information, we force the model to distill the essential information about our data in an information rich latent representation.\n",
    "\n",
    "\n",
    "### About the models and data\n",
    "\n",
    "    1. **The MNIST dataset**\n",
    "    In this notebook we are implementing different kinds of autoencoders, using the MNIST dataset, so you can get a feeling for what happens under the hood.\n",
    "    The MNIST dataset basically the \"hello world\" of machine learning, because it is a relatively small dataset, visually interpretable and thus easy to train,\n",
    "    diagnose and reason about. \n",
    "\n",
    "    2. **Autoencoders**\n",
    "    We are going to start with a very simple autoencoder, that simply takes our data, compresses it and decompresses it. We are then going to expand the complexity\n",
    "    of the models by introducing **convolutional layers** to take advantage of useful inductive biases in the data. To increase the robustness of the models we can then \n",
    "    make the task harder, by injecting noise into the data before we feed the data into the model (**Denoising Autoencoders**). Finally we are going to build a \n",
    "    **variational autoencoder** to increase the robustness of the autoencoders for data generation. All topics will be explained in more detail throughout the notebook.\n",
    "\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0ec1c-7b43-4b21-8068-809951636afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "script_path = os.getcwd()\n",
    "print(script_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b13c5-fa44-4071-bbbb-c62c3468b278",
   "metadata": {},
   "source": [
    "# Downloading the data\n",
    "\n",
    "\n",
    "We are starting of, by downloading the data and transforming it it to a tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd82438-cebd-4c6c-a223-6e2bbdc95fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_data = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Extract labels for stratified splitting\n",
    "targets = mnist_data.targets.numpy()  # MNIST labels\n",
    "\n",
    "# Perform initial stratified split for train+val and test\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    range(len(targets)),\n",
    "    test_size=0.2,  # 20% for testing\n",
    "    stratify=targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split train+val into train and val\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx,\n",
    "    test_size=0.25,  # 25% of train+val (or 20% of total data) for validation\n",
    "    stratify=[targets[i] for i in train_val_idx],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create subsets\n",
    "train_data = Subset(mnist_data, train_idx)\n",
    "val_data = Subset(mnist_data, val_idx)\n",
    "test_data = Subset(mnist_data, test_idx)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Validation size: {len(val_loader.dataset)}, Test size: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737336ac-152d-4bb8-bc37-a0d72fe186e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inspecting the data\n",
    "\n",
    "We see that the values range from 0 to 1. and it consists of hand written digits with the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f07f7-c1bc-48a8-beb4-4ed825f6baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "print(torch.min(images), torch.max(images))\n",
    "\n",
    "# Display the images\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45a91c-5bff-477b-8f52-50ec28ab6a1c",
   "metadata": {},
   "source": [
    "# Building the model (simple linear layer)\n",
    "\n",
    "For the autoencoder we are creating a simple feed forward neural network, with linear layers and hidden layers. The initial image shape will be (N, 784), where N is our batch size. In the init method we are using the super init, which initializes the super class.\n",
    "\n",
    "We first reduce the images with a few linear layers. To structure the code we are using a sequential model. We are creating a sequential model in which we continously apply linear layers. Remember: We noticed above, that values of the images were ranging from 0 to 1. Therefore, we need to have values from 0 to 1 as our ouput aswell. For this we are using the sigmoid function as the last activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df73472-6af3-4113-acf6-d8b4d63fc1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, d_z=10):\n",
    "        super().__init__()\n",
    "        self.d_z=d_z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),    # N,784 -> N,128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, d_z),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_z, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Sigmoid()  # Sigmoid to get values between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        z = self.encoder(x)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded.view(-1, 1, 28, 28)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "    def generate(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c480df-c8cd-4cb2-8fe7-89104ae6ffd9",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "We define the number of epochs (in this case just 10 for a quick test), we create a list to store the outputs. We start the training process by iterating over the epochs and then we iterate over our data loader.\n",
    "\n",
    "- while iterating over the data_loader, we are reshaping the images. The images are in shape 28*28 and we are reshaping them to 784.\n",
    "- We then call the model and get the reconstructed image (recon)\n",
    "- We thenn call the criterion with the reconstructed image and the original image (to calculate the mean squared error)\n",
    "- We are then resetting our gradiends \n",
    "- We are then calling the back propagation algorithm\n",
    "- Finally the optimization step\n",
    "\n",
    "We are also printing the epoch and the loss of the epoch at every step and append the epoch, the image and the reconstructed image in the outputs list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753691af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses, test_loss, best_epoch, num_epochs, model_name=\"model\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.axvline(x=best_epoch + 1, color='red', linestyle='--', label='Lowest Val Loss')\n",
    "    plt.scatter(best_epoch + 1, min(val_losses), color='red', zorder=5)  # Mark the point\n",
    "    plt.title('Training, Validation, and Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Save the best model\n",
    "    dest = os.path.join(script_path, f'plots/{model_name}_training_curve.png')\n",
    "    if not os.path.exists(os.path.dirname(dest)):\n",
    "        os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
    "    plt.savefig(dest)\n",
    "    plt.show()\n",
    "\n",
    "def add_noise(inputs):\n",
    "    noise = (0.1)*torch.randn_like(inputs)\n",
    "    return inputs + noise\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    test_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    num_epochs, \n",
    "    device, \n",
    "    use_noise=False, \n",
    "    add_noise_fn=None, \n",
    "    model_name=\"model\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with optional noise addition and save the best weights.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train (Autoencoder or VAE).\n",
    "        train_loader: DataLoader for the training set.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer.\n",
    "        num_epochs: Number of training epochs.\n",
    "        device: Device to train on (CPU or GPU).\n",
    "        use_noise: Whether to add noise to the input during training.\n",
    "        add_noise_fn: Function to add noise, if use_noise=True.\n",
    "        model_name: Name of the model for saving weights.\n",
    "\n",
    "    Returns:\n",
    "        train_losses, val_losses, test_loss, best_epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "    lowest_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for img, _ in train_loader:\n",
    "            img = img.to(device)\n",
    "\n",
    "            noisy_img = add_noise_fn(img) if use_noise and add_noise_fn else img\n",
    "\n",
    "            out = model(noisy_img)\n",
    "\n",
    "            # if recon is a tuple, use the first element (VAE)\n",
    "            if isinstance(out, tuple):\n",
    "                recon, mu, logvar = out\n",
    "                loss = criterion(recon, img, mu, logvar)\n",
    "            else:\n",
    "                loss = criterion(out, img) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img, _ in val_loader:\n",
    "                img = img.to(device)\n",
    "\n",
    "                noisy_img = add_noise_fn(img) if use_noise and add_noise_fn else img\n",
    "                \n",
    "                out = model(noisy_img)\n",
    "                # if out is a tuple, use the first element (VAE)\n",
    "                if isinstance(out, tuple):\n",
    "                    recon, mu, logvar = out\n",
    "                    loss = criterion(recon, img, mu, logvar)\n",
    "                else:\n",
    "                    loss = criterion(out, img) \n",
    "\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < lowest_val_loss:\n",
    "            lowest_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Testing\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, _ in test_loader:\n",
    "            img = img.to(device)\n",
    "\n",
    "            noisy_img = add_noise_fn(img) if use_noise and add_noise_fn else img\n",
    "            \n",
    "            out = model(noisy_img)\n",
    "            # if recon is a tuple, use the first element (VAE)\n",
    "            if isinstance(out, tuple):\n",
    "                recon, mu, logvar = out\n",
    "                loss = criterion(recon, img, mu, logvar)\n",
    "            else:\n",
    "                loss = criterion(out, img) \n",
    "                \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    dest = os.path.join(script_path, f'model_weights/{model_name}_best.pth')\n",
    "    if not os.path.exists(os.path.dirname(dest)):\n",
    "        os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
    "    torch.save(best_model_state, dest)\n",
    "\n",
    "    return train_losses, val_losses, test_losses, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02ea42-9e01-457a-a8c3-8da6cfc641c8",
   "metadata": {},
   "source": [
    "# Setting up the model\n",
    "\n",
    "In order to set up a model, we need a criterion to judge the performance of the model. In the case auf the encoder decoder, we are simply using mean square error loss (of the original image and the recread image).\n",
    "The second thing we need is an optimizer. For the optimizer we are simply using the adam optimizer. We have to give the optimizer something to optimize, which in our case are the model parameters and we need a learning rate, which will be set to 1e-3 (0.001) in this case, and we added a weight decay, which was set to 1e-5 (0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3f90f-205e-4759-97d5-b68b41e8e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs=20\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "d_z = 10\n",
    "autoencoder = Autoencoder(d_z=d_z).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "train_losses, val_losses, test_losses, best_epoch = train_model(\n",
    "    model=autoencoder,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5),\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    use_noise=False,                # Enable noise addition\n",
    "    add_noise_fn=add_noise,        # Pass the noise function\n",
    "    model_name=\"autoencoder\"     # Save weights with this name\n",
    ")\n",
    "\n",
    "plot_losses(train_losses, val_losses, test_losses, best_epoch, num_epochs, model_name=\"autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145f166-069d-4ca2-a7c8-890392c4bb96",
   "metadata": {},
   "source": [
    "## Training result\n",
    "\n",
    "We see that the loss decreased, so there is training happening. However, we also observe, that the loss is not perfectly low. To fix this we could for example increase the number of epochs.\n",
    "\n",
    "## Inspecting the reconstructions visually\n",
    "\n",
    "We can now take a look at the reconstructed images to see what is happening. We'll plot the images every 4 th epoch, where we get the images and the reconstructed images from the outputs array, which is a torch tensor that we have to convert back to a numpy object for plotting.\n",
    "\n",
    "We then iterate over the images and plot the first 9 images (remember to reshape in the reverse direction from 28*28 to 28,28). Finally the images are simply plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b4ceb-eae5-49cb-956d-4b074aa34ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, data_loader, device, num_samples=9, noise=False):\n",
    "    \"\"\"\n",
    "    Visualizes original images and their reconstructions from a trained model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained autoencoder model.\n",
    "        data_loader: DataLoader for the dataset to visualize.\n",
    "        device: Device to perform computation (CPU or GPU).\n",
    "        num_samples: Number of samples to visualize (default: 9).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of data\n",
    "        data_iter = iter(data_loader)\n",
    "        images, _ = next(data_iter)\n",
    "        images = images[:num_samples]  # Limit to the specified number of samples\n",
    "        images = images.to(device)\n",
    "\n",
    "        if noise:\n",
    "            images = add_noise(images)\n",
    "\n",
    "        # Generate reconstructions\n",
    "        recon = model(images)\n",
    "\n",
    "        # if recon is a tuple, use the first element (VAE)\n",
    "        if isinstance(recon, tuple):\n",
    "            recon = recon[0]\n",
    "\n",
    "        # Move data to CPU for visualization\n",
    "        images = images.cpu().numpy().squeeze() \n",
    "        recon = recon.cpu().numpy().squeeze() \n",
    "\n",
    "        # Plot original and reconstructed images\n",
    "        plt.figure(figsize=(18, 4))\n",
    "        plt.gray()\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Original images\n",
    "            plt.subplot(2, num_samples, i + 1)\n",
    "            plt.imshow(images[i], cmap='gray')\n",
    "            plt.title(\"Original\")\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Reconstructed images\n",
    "            plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "            plt.imshow(recon[i], cmap='gray')\n",
    "            plt.title(\"Reconstructed\")\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(autoencoder, test_loader, device, num_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeec57d-6bfb-4106-a918-cf483d5ede0f",
   "metadata": {},
   "source": [
    "# Building a convolutional neural network\n",
    "\n",
    "For the convolutional neural network, we are inputing the data not as a 784 vector, but instead as a 28, 28 shaped object, just like the image. For the sequential model we now want to use Conv2d layers instead of Linear layers. The Conv2d layers expect and input channel and an output channel. We are starting with 1 input channel and just chose 16 as the first number of output channels. Additionaly the Conv2d layer expects a kernel size, which we chose to be 3, a stride which was set to 2, and we have chosen to add a padding of 1.\n",
    "\n",
    "We start with one image with a size of 28, 28. After the first layer \"1 becomes 16\" and the size is reduced to 14, 14. For the next input channel, we need to use the output channel of the previous convolution. We increased the number of filters to 32. The output of the last layer will have 64 output channels, and we use a filter of the dimesion 7, 7 so the ouput size will be 1, 1 (basically one pixel, but 64 channels).\n",
    "\n",
    "For the decoding we apply the ConvTranspose 2D layers we again need to walk backwards by matching the output dimension with the input dimensios of the encoder.\n",
    "\n",
    "Note: Often when using convolutional neural networks, people use tha nn.MaxPool2d function, which will essentially reduce the size, to decode this you would need to use the nn.MaxUnpool2d function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1811802-2147-4aaf-ba9e-b44494f92402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_CNN(nn.Module):\n",
    "    def __init__(self, d_z=10):\n",
    "        super().__init__()\n",
    "        self.d_z = d_z\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),    # N, 16, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),   # N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7),                       # N, 64, 1, 1\n",
    "            nn.Flatten(),                               # N, 64\n",
    "            nn.Linear(64, d_z)                          # N, latent_dim (d_z)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_z, 64),                  # N, 64\n",
    "            nn.Unflatten(1, (64, 1, 1)),                # N, 64, 1, 1\n",
    "            nn.ConvTranspose2d(64, 32, 7),             # N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # N, 16, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  # N, 1, 28, 28\n",
    "            nn.Sigmoid()                                # Sigmoid to get values between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return recon\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "    \n",
    "    def generate(self, z):\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249b8ea-ad1e-45e8-9235-61fc23b68ccb",
   "metadata": {},
   "source": [
    "# Training\n",
    "We did not transform the image into a 1 dimensional vector, thats why we do not need a reshaping step in for the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b52060-53b8-48fb-83d0-a0c79b2e7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_cnn = Autoencoder_CNN(d_z=10).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder_cnn.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "train_losses, val_losses, test_losses, best_epoch = train_model(\n",
    "    model=autoencoder_cnn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    use_noise=False,                # Enable noise addition\n",
    "    add_noise_fn=add_noise,         # Pass the noise function\n",
    "    model_name=\"autoencoder_cnn\"        # Save weights with this name\n",
    ")\n",
    "\n",
    "plot_losses(train_losses, val_losses, test_losses, best_epoch, num_epochs, model_name=\"autoencoder_cnn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a90a05-0f86-4add-a844-63eefb90cd48",
   "metadata": {},
   "source": [
    "# CNN training results\n",
    "\n",
    "Now we see, that the loss is way superior to the Linear Neural Networks\n",
    "\n",
    "## CNN results visually\n",
    "\n",
    "Here we also don't need the reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b68326",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(autoencoder_cnn, test_loader, device, num_samples=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae76ab2-fc39-4878-bdc0-38b6b1d83670",
   "metadata": {},
   "source": [
    "# Denoising autoencoder\n",
    "\n",
    "When adding noise to an image before it passes through the first layers of the autoencoder,the autoencoder is forced to pick up the patterns rather, and can not just compress the image down and scale it back up again. For the denoising autoencoder we need a function that adds noise to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f82eb-f2a4-4dae-9c00-8da73838b2c6",
   "metadata": {},
   "source": [
    "## Visualizing noisy images\n",
    "\n",
    "we added noise from 1 to 0 which we mutiplied by 0.1, giving uns noise in the range of .1 to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1a9b9-2b89-4d74-aa4d-a5fd4dde9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply noise to the original images\n",
    "noised_images = add_noise(images)\n",
    "\n",
    "# Display the images\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "plt.gray()\n",
    "\n",
    "# Plot original images\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 10, idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx].cpu().numpy()), cmap='gray')\n",
    "    ax.set_title(f\"Label: {labels[idx].item()}\", fontsize=8)\n",
    "\n",
    "# Plot noised images\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 10, 10 + idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(noised_images[idx].cpu().numpy()), cmap='gray')\n",
    "    ax.set_title(f\"Noised\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41778d88-b651-4a1d-9472-56831ea75205",
   "metadata": {},
   "source": [
    "## Building a denoising CNN autoencoder\n",
    "\n",
    "For the denoising Autoencoder we are just reusing the CNN autoencoder, the difference will be in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d53fc0-728b-4352-aafb-90be217e01e4",
   "metadata": {},
   "source": [
    "## Training the denoiser CNN\n",
    "\n",
    "for the denoiser neural network we add noise to an image before it enters the neural network. However, the quality of the output will be judged how well the network reproduced the original image, without the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6d7b8-8387-46f8-a164-f03979e67553",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = Autoencoder_CNN(d_z=10).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(denoiser.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "train_losses, val_losses, test_losses, best_epoch = train_model(\n",
    "    model=denoiser,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    use_noise=True,                # Enable noise addition\n",
    "    add_noise_fn=add_noise,         \n",
    "    model_name=\"denoiser\"        \n",
    ")\n",
    "\n",
    "plot_losses(train_losses, val_losses, test_losses, best_epoch, num_epochs, model_name=\"denoiser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37928d24-6e58-44cc-ba00-00f717b9cee6",
   "metadata": {},
   "source": [
    "## Visual results of denoiser CNN\n",
    "\n",
    "We can observe, that the loss did not get as low as in the no noise case, but thre results still look very good. Adding more epochs could lead to further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f28a95-7cec-4bc0-ac5a-2db1766b75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(denoiser, test_loader, device, num_samples=9, noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd95df57",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, d_z=10):\n",
    "        super().__init__()\n",
    "        self.d_z = d_z\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),    # N, 16, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),   # N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7),                       # N, 64, 1, 1\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()                                # N, 64\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, d_z)          # Mean of the latent distribution\n",
    "        self.fc_logvar = nn.Linear(64, d_z)      # Log-variance of the latent distribution\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(d_z, 64)      # N, 64\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.Unflatten(1, (64, 1, 1)),                # N, 64, 1, 1\n",
    "            nn.ConvTranspose2d(64, 32, 7),             # N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # N, 16, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),  # N, 1, 28, 28\n",
    "            nn.Sigmoid()                                # Sigmoid to get values between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: sample z ~ N(mu, sigma^2).\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)                  # Standard deviation\n",
    "        eps = torch.randn_like(std)                   # Random noise\n",
    "        return mu + eps * std                          # Reparameterized sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        encoded = self.encoder_conv(x)\n",
    "        mu = self.fc_mu(encoded)\n",
    "        logvar = self.fc_logvar(encoded)\n",
    "        \n",
    "        # Sampling z\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decoding\n",
    "        reconstructed = self.decoder_conv(self.fc_decode(z))\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoding\n",
    "        encoded = self.encoder_conv(x)\n",
    "        mu = self.fc_mu(encoded)\n",
    "        logvar = self.fc_logvar(encoded)\n",
    "        \n",
    "        # Sampling z\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z\n",
    "\n",
    "    def generate(self, z):\n",
    "        return self.decoder_conv(self.fc_decode(z))\n",
    "    \n",
    "    def loss(self, reconstructed, x, mu, logvar):\n",
    "        # Reconstruction loss (Binary Cross-Entropy)\n",
    "        recon_loss = nn.functional.binary_cross_entropy(reconstructed, x, reduction=\"sum\")\n",
    "        \n",
    "        # KL divergence\n",
    "        q_z = Normal(mu, torch.exp(0.5 * logvar))      # Learned distribution\n",
    "        p_z = Normal(torch.zeros_like(mu), torch.ones_like(mu))  # Standard normal prior\n",
    "        kl_loss = kl_divergence(q_z, p_z).sum()\n",
    "        \n",
    "        return recon_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7cb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(d_z=10).to(device)\n",
    "criterion = vae.loss\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "num_epochs = 30\n",
    "\n",
    "train_losses, val_losses, test_losses, best_epoch = train_model(\n",
    "    model=vae,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    use_noise=True,                # Enable noise addition\n",
    "    add_noise_fn=add_noise,         \n",
    "    model_name=\"vae\"        \n",
    ")\n",
    "\n",
    "plot_losses(train_losses, val_losses, test_losses, best_epoch, num_epochs, model_name=\"vae\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(vae, test_loader, device, num_samples=9, noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a935fc08",
   "metadata": {},
   "source": [
    "# Comparing Latent Spaces and Generative Capabilities\n",
    "\n",
    "In this section, we compare the latent spaces learned by the autoencoders and their generative capabilities.\n",
    "\n",
    "## Latent Space Comparison\n",
    "\n",
    "Latent space comparison involves encoding a shared dataset (e.g., the MNIST test set) into the latent spaces of each autoencoder. We visualize these latent spaces using dimensionality reduction techniques like t-SNE or UMAP to understand the structure and separability of the encoded representations.\n",
    "\n",
    "## Generative Capability Comparison\n",
    "\n",
    "Generative capability comparison involves using the `generate` function of each autoencoder to produce samples. We qualitatively assess the fidelity and diversity of these generated samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dictionary of models\n",
    "autoencoders = {\n",
    "    \"Autoencoder\": autoencoder,\n",
    "    \"Autoencoder CNN\": autoencoder_cnn,\n",
    "    \"Denoiser\": denoiser,\n",
    "    \"VAE\": vae\n",
    "}\n",
    "\n",
    "# Collect images and labels\n",
    "test = [(img,label) for img, label in test_loader.dataset]\n",
    "imgs = torch.stack([img for img, _ in test], dim=0)\n",
    "labels = torch.tensor([label for _, label in test])\n",
    "\n",
    "# Encode data using each model\n",
    "encoded_data = {}\n",
    "for model_name, model in autoencoders.items():\n",
    "    encoded_data[model_name] = model.encode(imgs.to(device)).cpu().detach().numpy()\n",
    "\n",
    "# Apply UMAP to the original data\n",
    "original_data = imgs.view(imgs.size(0), -1).numpy()  # Flatten the images\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "original_data_2D = umap.fit_transform(original_data)\n",
    "\n",
    "# Apply UMAP to the latent spaces\n",
    "latent_spaces_2D = {name: umap.fit_transform(data) for name, data in encoded_data.items()}\n",
    "\n",
    "# Plot the UMAP results\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Plot for unencoded data\n",
    "plt.subplot(3, len(latent_spaces_2D) // 2 + 1, 1)\n",
    "for label in torch.unique(labels):\n",
    "    indices = (labels == label).numpy()\n",
    "    plt.scatter(original_data_2D[indices, 0], original_data_2D[indices, 1], s=2, label=f\"Label {label.item()}\", alpha=0.7)\n",
    "plt.title(\"Original Data\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot for latent spaces\n",
    "for i, (name, latent_2D) in enumerate(latent_spaces_2D.items(), start=2):\n",
    "    plt.subplot(3, len(latent_spaces_2D) // 2 + 1, i)\n",
    "    for label in torch.unique(labels):\n",
    "        indices = (labels == label).numpy()\n",
    "        plt.scatter(latent_2D[indices, 0], latent_2D[indices, 1], s=2, label=f\"Label {label.item()}\", alpha=0.7)\n",
    "    plt.title(f\"{name} Latent Space\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "\n",
    "# Generate new samples using each autoencoder's `generate` function\n",
    "generated_samples = {}\n",
    "for model_name, model in autoencoders.items():\n",
    "    # Sample latent vectors from a standard normal distribution\n",
    "    d_z = model.d_z\n",
    "    z = np.random.multivariate_normal(mean=np.zeros(d_z), cov=np.eye(d_z), size=num_samples)\n",
    "    \n",
    "    # Generate samples from the latent space\n",
    "    generated_samples[model_name] = model.generate(torch.tensor(z, dtype=torch.float32).to(device))\n",
    "\n",
    "# Visualize generated samples\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, (name, samples) in enumerate(generated_samples.items()):\n",
    "    for j, sample in enumerate(samples.cpu().detach().numpy()):\n",
    "        plt.subplot(len(generated_samples), 10, i * 10 + j + 1)\n",
    "        plt.imshow(sample.reshape(28, 28), cmap=\"gray\")  # Reshape to (28, 28) for visualization\n",
    "        plt.axis(\"off\")\n",
    "        if j == 0:\n",
    "            # center title for each row\n",
    "            plt.title(name)\n",
    "\n",
    "    plt.suptitle(f\"{name} Generated Samples\", y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3184341",
   "metadata": {},
   "source": [
    "# Final notes\n",
    "Here we can see the true importance of imposing regularizations of on the latent space. If we try to use the different autoencoders for data generation \n",
    "the VAE is the only viable option. Let's take a moment to reflect about why that is.\n",
    "\n",
    "Regular autoencoders are only encouraged to compress data to some arbitrary latent space. However, the model is not incentivised to create the latent space\n",
    "in a particular region in the vector space it creates. That means that the distances for latent representations of different digits might be placed far appart\n",
    "and the model does not really care about their relative position. A VAE on the other hand forces the latent space to stay within a desired region, in our case\n",
    "normally distributed around the origin. When we want to generate new data, this now gives us the freedom to simply sample random vectors from that region and \n",
    "send them through the decoder and we can expect reasonable results! I helped you to build an intuition why latent space geometry matters, and what the implications\n",
    "of the a good or bad geometry can be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25b994",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
